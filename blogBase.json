{"singlePage": [], "startSite": "", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "themeMode": "manual", "dayTheme": "light", "nightTheme": "dark", "urlMode": "pinyin", "script": "", "style": "", "head": "", "indexScript": "", "indexStyle": "", "bottomText": "", "showPostSource": 1, "iconList": {}, "UTC": 8, "rssSplit": "sentence", "exlink": {}, "needComment": 1, "allHead": "", "title": "My Blogs", "subTitle": "Just My Blogs", "avatarUrl": "https://avatars.githubusercontent.com/u/137168144", "GMEEK_VERSION": "last", "postListJson": {"P1": {"htmlDir": "docs/post/The Diffusion Principle Understood From The Perspective Of Convolution.html", "labels": ["blog"], "postTitle": "The Diffusion Principle Understood From The Perspective Of Convolution", "postUrl": "post/The%20Diffusion%20Principle%20Understood%20From%20The%20Perspective%20Of%20Convolution.html", "postSourceUrl": "https://github.com/Refound-445/Refound-445.github.io/issues/1", "commentNum": 0, "wordCount": 10536, "description": "# 1. What is Convolution?\r\n\r\n## 1.1 Understanding Convolution from a Mathematical Perspective\r\n\r\nThe mathematical formula for convolution is typically represented as the convolution of two functions, \\( f(x) \\) and \\( g(x) \\). It is defined as:\r\n\r\n$$\r\n(f * g)(x) = \\int_{-\\infty}^{\\infty} f(t) \\cdot g(x - t) \\, dt\r\n$$\r\n\r\nWhere:\r\n\r\n- \\( f(x) \\) and \\( g(x) \\) are the two functions to be convolved.\r\n- \\( (f * g)(x) \\) is the resulting function after the convolution.\r\n- \\( t \\) is the integration variable.\r\n\r\nFor discrete convolution, the formula is:\r\n\r\n$$\r\n(f * g)[n] = \\sum_{k=-\\infty}^{\\infty} f[k] \\cdot g[n - k]\r\n$$\r\n\r\nHere, \\( f[k] \\) and \\( g[k] \\) are discrete signals, and \\( n \\) is the discrete output index.\r\n\r\n## 1.2 Visualizing Convolution\r\n\r\nIn the image, the left part shows the pixel value matrix of a grayscale image (i.e., how the image is represented as numbers for a computer). In the middle is the **convolution kernel** matrix, which slides over the original image starting from the top-left corner. The kernel computes a value at each position, and this process is repeated across the image. The resulting values form the **right image (feature map)**, which contains the local features of the original image obtained through the convolution process.\r\n\r\n![Image here](https://i-blog.csdnimg.cn/direct/a822820b303d4eafa023d02a97898807.png)\r\n\r\nThe animation works like this: ![Image here](https://i-blog.csdnimg.cn/direct/e8f91c305ccc4be69a52862363545df2.webp)\r\n\r\n# 2. Diffusion Model Principles\r\n\r\n## 2.1 Principles of Early Generative Models\r\n\r\nEarly generative models such as GAN (Generative Adversarial Networks) and VAE (Variational Autoencoders) involve inverting the original model. For instance, in GAN, the recognition model is a conventional convolution network used for identifying generated images. The generation model, however, reverses this by using a transposed convolution network (also called deconvolution) to generate images, but this approach doesn't produce ideal results.\r\n\r\nLet's talk about transposed convolution. Transposed convolution is the reverse of convolution: convolution turns a large matrix into a smaller one, whereas transposed convolution takes a smaller matrix and generates a larger one. As shown in the image below, it creates the dashed-line matrix! ![](https://i-blog.csdnimg.cn/direct/a71b44ad44434fd78a6b32e2393065b7.gif)\r\n\r\n## 2.2 Diffusion Model\r\n\r\nWhile directly generating images is not ideal, scientists have drawn inspiration from diffusion in physics. In nature, substances tend to move toward a disordered state. For example, when a drop of ink is added to a glass of water, it gradually spreads out. This suggests that generative models could also take a gradual, step-by-step approach instead of rushing, aiming for steady progress.\r\n\r\nThus, diffusion models were born. We start by adding noise to an image\u2019s pixels, which results in a very chaotic image. Conversely, we can also reverse this process to recover the original image from this noisy one.\r\n\r\n![Image here](https://i-blog.csdnimg.cn/direct/6c1dfe49dc5941c797c0a8868be5b79b.png)\r\n\r\n## 2.3 Convolution in Diffusion Models\r\n\r\nDiffusion models typically use a UNet network to predict denoised images, with the addition of **timestep** to reflect the noise level. The prediction is done for each **timestep** of the image.\r\n\r\nAs shown in the image, this is a convolution kernel from the UNet network used in diffusion (with code implementation to follow). In fact, throughout the entire network, the properties of the convolution kernel remain largely unchanged, and the input's width and height do not change during the forward pass. Only the number of channels changes.\r\n\r\n![Image here](https://i-blog.csdnimg.cn/direct/76389456b2a8436b8d38156e22b7326f.png)\r\nWe remembered, **convolution** maps a matrix onto a feature matrix, while **diffusion** introduces disorder into the matrix.Try to think of it this way: **convolution** disturbs or restores the local features of a matrix, while **diffusion** relies on **convolution** to diffuse local features.\r\n\r\n![Image here](https://i-blog.csdnimg.cn/direct/1066b9f4fd8c4f3fb70c350e6679c624.png)\r\n\r\n# 3. Code Implementation of Diffusion\r\n\r\nTheory is one thing, but let\u2019s dive into a practical example.\r\n\r\n## 3.1 Importing Required Libraries\r\n\r\n```python\r\nimport torch\r\nimport torchvision\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom diffusers import DDPMScheduler, UNet2DModel\r\nfrom matplotlib import pyplot as plt\r\nimport os\r\n\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f'Using device: {device}')\r\n```\r\n\r\n## 3.2 Using the MNIST Dataset\r\n\r\n```python\r\ndataset = torchvision.datasets.MNIST(\r\n    root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor()\r\n)\r\ntrain_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\r\n```\r\n\r\n## 3.3 Writing the Noise Corruption Formula\r\n\r\nCorrupting means mixing the image with noise in a certain proportion to achieve denoising. As the diffusion process progresses, the image becomes clearer, and the noise has less of an effect.\r\n\r\n```python\r\ndef corrupt(x, amount):\r\n    '''Corrupt the input `x` by mixing it with noise according to `amount`'''\r\n    noise = torch.rand_like(x)\r\n    amount = amount.view(-1, 1, 1, 1)  # Adjust shape for broadcasting\r\n    return x * (1 - amount) + noise * amount\r\n```\r\n\r\n## 3.4 Creating a Simple UNet Model\r\n\r\nWe\u2019ll use a mini UNet model (not the standard one) that still achieves good results.\r\n\r\n```python\r\nclass BasicUNet(nn.Module):\r\n    '''A minimal UNet implementation.'''\r\n\r\n    def __init__(self, in_channels=1, out_channels=1):\r\n        super().__init__()\r\n        self.down_layers = torch.nn.ModuleList(\r\n            [\r\n                nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),\r\n                nn.Conv2d(32, 64, kernel_size=5, padding=2),\r\n                nn.Conv2d(64, 64, kernel_size=5, padding=2),\r\n            ]\r\n        )\r\n        self.up_layers = torch.nn.ModuleList(\r\n            [\r\n                nn.Conv2d(64, 64, kernel_size=5, padding=2),\r\n                nn.Conv2d(64, 32, kernel_size=5, padding=2),\r\n                nn.Conv2d(32, out_channels, kernel_size=5, padding=2),\r\n            ]\r\n        )\r\n        self.act = nn.SiLU()  # Activation function\r\n        self.downscale = nn.MaxPool2d(2)\r\n        self.upscale = nn.Upsample(scale_factor=2)\r\n\r\n    def forward(self, x):\r\n        h = []\r\n        for i, l in enumerate(self.down_layers):\r\n            x = self.act(l(x))  # Pass through the layer and activation function\r\n            if i < 2:  # Skip connection for all but the final down layer\r\n                h.append(x)  # Store output for skip connection\r\n                x = self.downscale(x)  # Downscale for next layer\r\n\r\n        for i, l in enumerate(self.up_layers):\r\n            if i > 0:  # For all but the first up layer\r\n                x = self.upscale(x)  # Upscale\r\n                x += h.pop()  # Use stored output (skip connection)\r\n            x = self.act(l(x))  # Pass through the layer and activation function\r\n\r\n        return x\r\n```\r\n\r\nThe network looks like this:\r\n\r\n![Image here](https://i-blog.csdnimg.cn/direct/9d885023e67d43dd84c7e5b27bd770c9.png)\r\n\r\n## 3.5 Defining the Training Parameters\r\n\r\n```python\r\n# Dataloader (adjust batch size)\r\nbatch_size = 128\r\ntrain_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\r\n\r\n# Number of epochs\r\nn_epochs = 30\r\n\r\n# Create the network\r\nnet = UNet2DModel(\r\n    sample_size=28,  # Target image resolution\r\n    in_channels=1,  # Input channels, 3 for RGB images\r\n    out_channels=1,  # Output channels\r\n    layers_per_block=2,  # ResNet layers per UNet block\r\n    block_out_channels=(32, 64, 64),  # Matching our basic UNet example\r\n    down_block_types=(\r\n        'DownBlock2D',  # Regular ResNet downsampling block\r\n        'AttnDownBlock2D',  # ResNet block with attention\r\n        'AttnDownBlock2D',\r\n    ),\r\n    up_block_types=(\r\n        'AttnUpBlock2D',\r\n        'AttnUpBlock2D',  # ResNet block with attention\r\n        'UpBlock2D',  # Regular ResNet upsampling block\r\n    ),\r\n)\r\nnet.to(device)\r\n\r\n# Loss function\r\nloss_fn = nn.MSELoss()\r\n\r\n# Optimizer\r\nopt = torch.optim.Adam(net.parameters(), lr=1e-3)\r\n\r\n# Track losses\r\nlosses = []\r\n\r\n# Training loop\r\nfor epoch in range(n_epochs):\r\n    for x, y in train_dataloader:\r\n        x = x.to(device)  # Data on GPU\r\n        noise_amount = torch.rand(x.shape[0]).to(device)  # Random noise amount\r\n        noisy_x = corrupt(x, noise_amount)  # Create noisy input\r\n\r\n        # Get model prediction\r\n        pred = net(noisy_x, 0).sample  # Use timestep 0\r\n\r\n        # Calculate the loss\r\n        loss = loss_fn(pred, x)  # Compare to original clean image\r\n\r\n        # Backprop and update parameters\r\n        opt.zero_grad()\r\n        loss.backward()\r\n        opt.step()\r\n\r\n        losses.append(loss.item())\r\n\r\n    avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)\r\n    print(f'Epoch {epoch}. Average loss: {avg_loss:.5f}')\r\n```\r\n\r\n## 3.6 Testing the Model\r\n\r\nAfter training, we can test the diffusion model\u2019s power:\r\n\r\n```python\r\nn_steps = 40\r\nx = torch.rand(8, 1, 28, 28).to(device)  # Start from random noise\r\nstep_history = [x.detach().cpu()]\r\npred_output_history = []\r\n\r\nfor i in range(n_steps):\r\n    with torch.no_grad():  # No gradients during inference\r\n        pred = net(x,0).sample  # Predict denoised image\r\n    pred_output_history.append(pred.detach().cpu())  # Store prediction\r\n    mix_factor = 1 / (n_steps - i)  # Mix towards prediction\r\n    x = x * (1 - mix_factor) + pred * mix_factor  # Move partway to the denoised image\r\n    step_history.append(x.detach().cpu())  # Store for plotting\r\n\r\nfig, axs = plt.subplots(n_steps, 2, figsize=(9, 32), sharex=True)\r\naxs[0, 0].set_title('Input (noisy)')\r\naxs[0, 1].set_title('Model Prediction')\r\nfor i in range(n_steps):\r\n    axs[i, 0].imshow(torchvision.utils.make_grid(step_history[i])[0].clip(0, 1), cmap='Greys')\r\n    axs[i, 1].imshow(torchvision.utils.make_grid(pred_output_history[i])[0].clip(0, 1), cmap='Greys')\r\n```\r\n\r\n![Image here](https://i-blog.csdnimg.cn/direct/84fa926fde4146a6812053b5d88f366b.png)\r\n\r\nThis shows the final results of testing the model, and the output looks quite impressive!\r\n\r\n# Conclusion\r\n\r\nThis concludes the explanation of the diffusion model. You can try modifying the UNet network or adjusting parameters to see if you can achieve even more remarkable results!\u3002", "top": 0, "createdAt": 1736964950, "style": "", "script": "<script>MathJax = {tex: {inlineMath: [[\"$\", \"$\"]]}};</script><script async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/137168144", "createdDate": "2025-01-16", "dateLabelColor": "#0969da"}}, "singeListJson": {}, "labelColorDict": {"blog": "#67E6AB", "bug": "#d73a4a", "documentation": "#0075ca", "duplicate": "#cfd3d7", "enhancement": "#a2eeef", "good first issue": "#7057ff", "help wanted": "#008672", "invalid": "#e4e669", "question": "#d876e3", "wontfix": "#ffffff"}, "displayTitle": "My Blogs", "faviconUrl": "https://avatars.githubusercontent.com/u/137168144", "ogImage": "https://avatars.githubusercontent.com/u/137168144", "primerCSS": "<link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />", "homeUrl": "https://Refound-445.github.io", "prevUrl": "disabled", "nextUrl": "disabled"}