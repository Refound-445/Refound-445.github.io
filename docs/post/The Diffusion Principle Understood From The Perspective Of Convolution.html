<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://avatars.githubusercontent.com/u/137168144"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="# Table of Contents

- [1. What is Convolution?](#1-what-is-convolution)
  - [1.1 Understanding Convolution from a Mathematical Perspective](#11-understanding-convolution-from-a-mathematical-perspective)
  - [1.2 Visualizing Convolution](#12-visualizing-convolution)

- [2. Diffusion Model Principles](#2-diffusion-model-principles)
  - [2.1 Principles of Early Generative Models](#21-principles-of-early-generative-models)
  - [2.2 Diffusion Model](#22-diffusion-model)
  - [2.3 Convolution in Diffusion Models](#23-convolution-in-diffusion-models)

- [3. Code Implementation of Diffusion](#3-code-implementation-of-diffusion)
  - [3.1 Importing Required Libraries](#31-importing-required-libraries)
  - [3.2 Using the MNIST Dataset](#32-using-the-mnist-dataset)
  - [3.3 Writing the Noise Corruption Formula](#33-writing-the-noise-corruption-formula)
  - [3.4 Creating a Simple UNet Model](#34-creating-a-simple-unet-model)
  - [3.5 Defining the Training Parameters](#35-defining-the-training-parameters)
  - [3.6 Testing the Model](#36-testing-the-model)
- [Conclusion](#Conclusion)
# 1. What is Convolution?

## 1.1 Understanding Convolution from a Mathematical Perspective

The mathematical formula for convolution is typically represented as the convolution of two functions, \( f(x) \) and \( g(x) \). It is defined as:

$$
(f * g)(x) = \int_{-\infty}^{\infty} f(t) \cdot g(x - t) \, dt
$$

Where:

- \( f(x) \) and \( g(x) \) are the two functions to be convolved.
- \( (f * g)(x) \) is the resulting function after the convolution.
- \( t \) is the integration variable.

For discrete convolution, the formula is:

$$
(f * g)[n] = \sum_{k=-\infty}^{\infty} f[k] \cdot g[n - k]
$$

Here, \( f[k] \) and \( g[k] \) are discrete signals, and \( n \) is the discrete output index.

## 1.2 Visualizing Convolution

In the image, the left part shows the pixel value matrix of a grayscale image (i.e., how the image is represented as numbers for a computer). In the middle is the **convolution kernel** matrix, which slides over the original image starting from the top-left corner. The kernel computes a value at each position, and this process is repeated across the image. The resulting values form the **right image (feature map)**, which contains the local features of the original image obtained through the convolution process.

![Image here](https://i-blog.csdnimg.cn/direct/a822820b303d4eafa023d02a97898807.png)

The animation works like this: ![Image here](https://i-blog.csdnimg.cn/direct/e8f91c305ccc4be69a52862363545df2.webp)

# 2. Diffusion Model Principles

## 2.1 Principles of Early Generative Models

Early generative models such as GAN (Generative Adversarial Networks) and VAE (Variational Autoencoders) involve inverting the original model. For instance, in GAN, the recognition model is a conventional convolution network used for identifying generated images. The generation model, however, reverses this by using a transposed convolution network (also called deconvolution) to generate images, but this approach doesn't produce ideal results.

Let's talk about transposed convolution. Transposed convolution is the reverse of convolution: convolution turns a large matrix into a smaller one, whereas transposed convolution takes a smaller matrix and generates a larger one. As shown in the image below, it creates the dashed-line matrix! ![](https://i-blog.csdnimg.cn/direct/a71b44ad44434fd78a6b32e2393065b7.gif)

## 2.2 Diffusion Model

While directly generating images is not ideal, scientists have drawn inspiration from diffusion in physics. In nature, substances tend to move toward a disordered state. For example, when a drop of ink is added to a glass of water, it gradually spreads out. This suggests that generative models could also take a gradual, step-by-step approach instead of rushing, aiming for steady progress.

Thus, diffusion models were born. We start by adding noise to an image’s pixels, which results in a very chaotic image. Conversely, we can also reverse this process to recover the original image from this noisy one.

![Image here](https://i-blog.csdnimg.cn/direct/6c1dfe49dc5941c797c0a8868be5b79b.png)

## 2.3 Convolution in Diffusion Models

Diffusion models typically use a UNet network to predict denoised images, with the addition of **timestep** to reflect the noise level. The prediction is done for each **timestep** of the image.

As shown in the image, this is a convolution kernel from the UNet network used in diffusion (with code implementation to follow). In fact, throughout the entire network, the properties of the convolution kernel remain largely unchanged, and the input's width and height do not change during the forward pass. Only the number of channels changes.

![Image here](https://i-blog.csdnimg.cn/direct/76389456b2a8436b8d38156e22b7326f.png)
We remembered, **convolution** maps a matrix onto a feature matrix, while **diffusion** introduces disorder into the matrix.Try to think of it this way: **convolution** disturbs or restores the local features of a matrix, while **diffusion** relies on **convolution** to diffuse local features.

![Image here](https://i-blog.csdnimg.cn/direct/1066b9f4fd8c4f3fb70c350e6679c624.png)

# 3. Code Implementation of Diffusion

Theory is one thing, but let’s dive into a practical example.

## 3.1 Importing Required Libraries

```python
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader
from diffusers import DDPMScheduler, UNet2DModel
from matplotlib import pyplot as plt
import os

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')
```

## 3.2 Using the MNIST Dataset

```python
dataset = torchvision.datasets.MNIST(
    root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor()
)
train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)
```

## 3.3 Writing the Noise Corruption Formula

Corrupting means mixing the image with noise in a certain proportion to achieve denoising. As the diffusion process progresses, the image becomes clearer, and the noise has less of an effect.

```python
def corrupt(x, amount):
    '''Corrupt the input `x` by mixing it with noise according to `amount`'''
    noise = torch.rand_like(x)
    amount = amount.view(-1, 1, 1, 1)  # Adjust shape for broadcasting
    return x * (1 - amount) + noise * amount
```

## 3.4 Creating a Simple UNet Model

We’ll use a mini UNet model (not the standard one) that still achieves good results.

```python
class BasicUNet(nn.Module):
    '''A minimal UNet implementation.'''

    def __init__(self, in_channels=1, out_channels=1):
        super().__init__()
        self.down_layers = torch.nn.ModuleList(
            [
                nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),
                nn.Conv2d(32, 64, kernel_size=5, padding=2),
                nn.Conv2d(64, 64, kernel_size=5, padding=2),
            ]
        )
        self.up_layers = torch.nn.ModuleList(
            [
                nn.Conv2d(64, 64, kernel_size=5, padding=2),
                nn.Conv2d(64, 32, kernel_size=5, padding=2),
                nn.Conv2d(32, out_channels, kernel_size=5, padding=2),
            ]
        )
        self.act = nn.SiLU()  # Activation function
        self.downscale = nn.MaxPool2d(2)
        self.upscale = nn.Upsample(scale_factor=2)

    def forward(self, x):
        h = []
        for i, l in enumerate(self.down_layers):
            x = self.act(l(x))  # Pass through the layer and activation function
            if i < 2:  # Skip connection for all but the final down layer
                h.append(x)  # Store output for skip connection
                x = self.downscale(x)  # Downscale for next layer

        for i, l in enumerate(self.up_layers):
            if i > 0:  # For all but the first up layer
                x = self.upscale(x)  # Upscale
                x += h.pop()  # Use stored output (skip connection)
            x = self.act(l(x))  # Pass through the layer and activation function

        return x
```

The network looks like this:

![Image here](https://i-blog.csdnimg.cn/direct/9d885023e67d43dd84c7e5b27bd770c9.png)

## 3.5 Defining the Training Parameters

```python
# Dataloader (adjust batch size)
batch_size = 128
train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Number of epochs
n_epochs = 30

# Create the network
net = UNet2DModel(
    sample_size=28,  # Target image resolution
    in_channels=1,  # Input channels, 3 for RGB images
    out_channels=1,  # Output channels
    layers_per_block=2,  # ResNet layers per UNet block
    block_out_channels=(32, 64, 64),  # Matching our basic UNet example
    down_block_types=(
        'DownBlock2D',  # Regular ResNet downsampling block
        'AttnDownBlock2D',  # ResNet block with attention
        'AttnDownBlock2D',
    ),
    up_block_types=(
        'AttnUpBlock2D',
        'AttnUpBlock2D',  # ResNet block with attention
        'UpBlock2D',  # Regular ResNet upsampling block
    ),
)
net.to(device)

# Loss function
loss_fn = nn.MSELoss()

# Optimizer
opt = torch.optim.Adam(net.parameters(), lr=1e-3)

# Track losses
losses = []

# Training loop
for epoch in range(n_epochs):
    for x, y in train_dataloader:
        x = x.to(device)  # Data on GPU
        noise_amount = torch.rand(x.shape[0]).to(device)  # Random noise amount
        noisy_x = corrupt(x, noise_amount)  # Create noisy input

        # Get model prediction
        pred = net(noisy_x, 0).sample  # Use timestep 0

        # Calculate the loss
        loss = loss_fn(pred, x)  # Compare to original clean image

        # Backprop and update parameters
        opt.zero_grad()
        loss.backward()
        opt.step()

        losses.append(loss.item())

    avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)
    print(f'Epoch {epoch}. Average loss: {avg_loss:.5f}')
```

## 3.6 Testing the Model

After training, we can test the diffusion model’s power:

```python
n_steps = 40
x = torch.rand(8, 1, 28, 28).to(device)  # Start from random noise
step_history = [x.detach().cpu()]
pred_output_history = []

for i in range(n_steps):
    with torch.no_grad():  # No gradients during inference
        pred = net(x,0).sample  # Predict denoised image
    pred_output_history.append(pred.detach().cpu())  # Store prediction
    mix_factor = 1 / (n_steps - i)  # Mix towards prediction
    x = x * (1 - mix_factor) + pred * mix_factor  # Move partway to the denoised image
    step_history.append(x.detach().cpu())  # Store for plotting

fig, axs = plt.subplots(n_steps, 2, figsize=(9, 32), sharex=True)
axs[0, 0].set_title('Input (noisy)')
axs[0, 1].set_title('Model Prediction')
for i in range(n_steps):
    axs[i, 0].imshow(torchvision.utils.make_grid(step_history[i])[0].clip(0, 1), cmap='Greys')
    axs[i, 1].imshow(torchvision.utils.make_grid(pred_output_history[i])[0].clip(0, 1), cmap='Greys')
```

![Image here](https://i-blog.csdnimg.cn/direct/84fa926fde4146a6812053b5d88f366b.png)

This shows the final results of testing the model, and the output looks quite impressive!

# Conclusion

This concludes the explanation of the diffusion model. You can try modifying the UNet network or adjusting parameters to see if you can achieve even more remarkable results!。">
<meta property="og:title" content="The Diffusion Principle Understood From The Perspective Of Convolution">
<meta property="og:description" content="# Table of Contents

- [1. What is Convolution?](#1-what-is-convolution)
  - [1.1 Understanding Convolution from a Mathematical Perspective](#11-understanding-convolution-from-a-mathematical-perspective)
  - [1.2 Visualizing Convolution](#12-visualizing-convolution)

- [2. Diffusion Model Principles](#2-diffusion-model-principles)
  - [2.1 Principles of Early Generative Models](#21-principles-of-early-generative-models)
  - [2.2 Diffusion Model](#22-diffusion-model)
  - [2.3 Convolution in Diffusion Models](#23-convolution-in-diffusion-models)

- [3. Code Implementation of Diffusion](#3-code-implementation-of-diffusion)
  - [3.1 Importing Required Libraries](#31-importing-required-libraries)
  - [3.2 Using the MNIST Dataset](#32-using-the-mnist-dataset)
  - [3.3 Writing the Noise Corruption Formula](#33-writing-the-noise-corruption-formula)
  - [3.4 Creating a Simple UNet Model](#34-creating-a-simple-unet-model)
  - [3.5 Defining the Training Parameters](#35-defining-the-training-parameters)
  - [3.6 Testing the Model](#36-testing-the-model)
- [Conclusion](#Conclusion)
# 1. What is Convolution?

## 1.1 Understanding Convolution from a Mathematical Perspective

The mathematical formula for convolution is typically represented as the convolution of two functions, \( f(x) \) and \( g(x) \). It is defined as:

$$
(f * g)(x) = \int_{-\infty}^{\infty} f(t) \cdot g(x - t) \, dt
$$

Where:

- \( f(x) \) and \( g(x) \) are the two functions to be convolved.
- \( (f * g)(x) \) is the resulting function after the convolution.
- \( t \) is the integration variable.

For discrete convolution, the formula is:

$$
(f * g)[n] = \sum_{k=-\infty}^{\infty} f[k] \cdot g[n - k]
$$

Here, \( f[k] \) and \( g[k] \) are discrete signals, and \( n \) is the discrete output index.

## 1.2 Visualizing Convolution

In the image, the left part shows the pixel value matrix of a grayscale image (i.e., how the image is represented as numbers for a computer). In the middle is the **convolution kernel** matrix, which slides over the original image starting from the top-left corner. The kernel computes a value at each position, and this process is repeated across the image. The resulting values form the **right image (feature map)**, which contains the local features of the original image obtained through the convolution process.

![Image here](https://i-blog.csdnimg.cn/direct/a822820b303d4eafa023d02a97898807.png)

The animation works like this: ![Image here](https://i-blog.csdnimg.cn/direct/e8f91c305ccc4be69a52862363545df2.webp)

# 2. Diffusion Model Principles

## 2.1 Principles of Early Generative Models

Early generative models such as GAN (Generative Adversarial Networks) and VAE (Variational Autoencoders) involve inverting the original model. For instance, in GAN, the recognition model is a conventional convolution network used for identifying generated images. The generation model, however, reverses this by using a transposed convolution network (also called deconvolution) to generate images, but this approach doesn't produce ideal results.

Let's talk about transposed convolution. Transposed convolution is the reverse of convolution: convolution turns a large matrix into a smaller one, whereas transposed convolution takes a smaller matrix and generates a larger one. As shown in the image below, it creates the dashed-line matrix! ![](https://i-blog.csdnimg.cn/direct/a71b44ad44434fd78a6b32e2393065b7.gif)

## 2.2 Diffusion Model

While directly generating images is not ideal, scientists have drawn inspiration from diffusion in physics. In nature, substances tend to move toward a disordered state. For example, when a drop of ink is added to a glass of water, it gradually spreads out. This suggests that generative models could also take a gradual, step-by-step approach instead of rushing, aiming for steady progress.

Thus, diffusion models were born. We start by adding noise to an image’s pixels, which results in a very chaotic image. Conversely, we can also reverse this process to recover the original image from this noisy one.

![Image here](https://i-blog.csdnimg.cn/direct/6c1dfe49dc5941c797c0a8868be5b79b.png)

## 2.3 Convolution in Diffusion Models

Diffusion models typically use a UNet network to predict denoised images, with the addition of **timestep** to reflect the noise level. The prediction is done for each **timestep** of the image.

As shown in the image, this is a convolution kernel from the UNet network used in diffusion (with code implementation to follow). In fact, throughout the entire network, the properties of the convolution kernel remain largely unchanged, and the input's width and height do not change during the forward pass. Only the number of channels changes.

![Image here](https://i-blog.csdnimg.cn/direct/76389456b2a8436b8d38156e22b7326f.png)
We remembered, **convolution** maps a matrix onto a feature matrix, while **diffusion** introduces disorder into the matrix.Try to think of it this way: **convolution** disturbs or restores the local features of a matrix, while **diffusion** relies on **convolution** to diffuse local features.

![Image here](https://i-blog.csdnimg.cn/direct/1066b9f4fd8c4f3fb70c350e6679c624.png)

# 3. Code Implementation of Diffusion

Theory is one thing, but let’s dive into a practical example.

## 3.1 Importing Required Libraries

```python
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader
from diffusers import DDPMScheduler, UNet2DModel
from matplotlib import pyplot as plt
import os

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')
```

## 3.2 Using the MNIST Dataset

```python
dataset = torchvision.datasets.MNIST(
    root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor()
)
train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)
```

## 3.3 Writing the Noise Corruption Formula

Corrupting means mixing the image with noise in a certain proportion to achieve denoising. As the diffusion process progresses, the image becomes clearer, and the noise has less of an effect.

```python
def corrupt(x, amount):
    '''Corrupt the input `x` by mixing it with noise according to `amount`'''
    noise = torch.rand_like(x)
    amount = amount.view(-1, 1, 1, 1)  # Adjust shape for broadcasting
    return x * (1 - amount) + noise * amount
```

## 3.4 Creating a Simple UNet Model

We’ll use a mini UNet model (not the standard one) that still achieves good results.

```python
class BasicUNet(nn.Module):
    '''A minimal UNet implementation.'''

    def __init__(self, in_channels=1, out_channels=1):
        super().__init__()
        self.down_layers = torch.nn.ModuleList(
            [
                nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),
                nn.Conv2d(32, 64, kernel_size=5, padding=2),
                nn.Conv2d(64, 64, kernel_size=5, padding=2),
            ]
        )
        self.up_layers = torch.nn.ModuleList(
            [
                nn.Conv2d(64, 64, kernel_size=5, padding=2),
                nn.Conv2d(64, 32, kernel_size=5, padding=2),
                nn.Conv2d(32, out_channels, kernel_size=5, padding=2),
            ]
        )
        self.act = nn.SiLU()  # Activation function
        self.downscale = nn.MaxPool2d(2)
        self.upscale = nn.Upsample(scale_factor=2)

    def forward(self, x):
        h = []
        for i, l in enumerate(self.down_layers):
            x = self.act(l(x))  # Pass through the layer and activation function
            if i < 2:  # Skip connection for all but the final down layer
                h.append(x)  # Store output for skip connection
                x = self.downscale(x)  # Downscale for next layer

        for i, l in enumerate(self.up_layers):
            if i > 0:  # For all but the first up layer
                x = self.upscale(x)  # Upscale
                x += h.pop()  # Use stored output (skip connection)
            x = self.act(l(x))  # Pass through the layer and activation function

        return x
```

The network looks like this:

![Image here](https://i-blog.csdnimg.cn/direct/9d885023e67d43dd84c7e5b27bd770c9.png)

## 3.5 Defining the Training Parameters

```python
# Dataloader (adjust batch size)
batch_size = 128
train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Number of epochs
n_epochs = 30

# Create the network
net = UNet2DModel(
    sample_size=28,  # Target image resolution
    in_channels=1,  # Input channels, 3 for RGB images
    out_channels=1,  # Output channels
    layers_per_block=2,  # ResNet layers per UNet block
    block_out_channels=(32, 64, 64),  # Matching our basic UNet example
    down_block_types=(
        'DownBlock2D',  # Regular ResNet downsampling block
        'AttnDownBlock2D',  # ResNet block with attention
        'AttnDownBlock2D',
    ),
    up_block_types=(
        'AttnUpBlock2D',
        'AttnUpBlock2D',  # ResNet block with attention
        'UpBlock2D',  # Regular ResNet upsampling block
    ),
)
net.to(device)

# Loss function
loss_fn = nn.MSELoss()

# Optimizer
opt = torch.optim.Adam(net.parameters(), lr=1e-3)

# Track losses
losses = []

# Training loop
for epoch in range(n_epochs):
    for x, y in train_dataloader:
        x = x.to(device)  # Data on GPU
        noise_amount = torch.rand(x.shape[0]).to(device)  # Random noise amount
        noisy_x = corrupt(x, noise_amount)  # Create noisy input

        # Get model prediction
        pred = net(noisy_x, 0).sample  # Use timestep 0

        # Calculate the loss
        loss = loss_fn(pred, x)  # Compare to original clean image

        # Backprop and update parameters
        opt.zero_grad()
        loss.backward()
        opt.step()

        losses.append(loss.item())

    avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)
    print(f'Epoch {epoch}. Average loss: {avg_loss:.5f}')
```

## 3.6 Testing the Model

After training, we can test the diffusion model’s power:

```python
n_steps = 40
x = torch.rand(8, 1, 28, 28).to(device)  # Start from random noise
step_history = [x.detach().cpu()]
pred_output_history = []

for i in range(n_steps):
    with torch.no_grad():  # No gradients during inference
        pred = net(x,0).sample  # Predict denoised image
    pred_output_history.append(pred.detach().cpu())  # Store prediction
    mix_factor = 1 / (n_steps - i)  # Mix towards prediction
    x = x * (1 - mix_factor) + pred * mix_factor  # Move partway to the denoised image
    step_history.append(x.detach().cpu())  # Store for plotting

fig, axs = plt.subplots(n_steps, 2, figsize=(9, 32), sharex=True)
axs[0, 0].set_title('Input (noisy)')
axs[0, 1].set_title('Model Prediction')
for i in range(n_steps):
    axs[i, 0].imshow(torchvision.utils.make_grid(step_history[i])[0].clip(0, 1), cmap='Greys')
    axs[i, 1].imshow(torchvision.utils.make_grid(pred_output_history[i])[0].clip(0, 1), cmap='Greys')
```

![Image here](https://i-blog.csdnimg.cn/direct/84fa926fde4146a6812053b5d88f366b.png)

This shows the final results of testing the model, and the output looks quite impressive!

# Conclusion

This concludes the explanation of the diffusion model. You can try modifying the UNet network or adjusting parameters to see if you can achieve even more remarkable results!。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://Refound-445.github.io/post/The%20Diffusion%20Principle%20Understood%20From%20The%20Perspective%20Of%20Convolution.html">
<meta property="og:image" content="https://avatars.githubusercontent.com/u/137168144">
<title>The Diffusion Principle Understood From The Perspective Of Convolution</title>
<link href="//unpkg.com/@wooorm/starry-night@2.1.1/style/both.css" rel="stylesheet" />


</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
.copy-feedback {
    display: none;
    position: absolute;
    top: 10px;
    right: 50px;
    color: var(--color-fg-on-emphasis);
    background-color: var(--color-fg-muted);
    border-radius: 3px;
    padding: 5px 8px;
    font-size: 12px;
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">The Diffusion Principle Understood From The Perspective Of Convolution</h1>
<div class="title-right">
    <a href="https://Refound-445.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/Refound-445/Refound-445.github.io/issues/1" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h1>Table of Contents</h1>
<ul>
<li>
<p><a href="#1-what-is-convolution">1. What is Convolution?</a></p>
<ul>
<li><a href="#11-understanding-convolution-from-a-mathematical-perspective">1.1 Understanding Convolution from a Mathematical Perspective</a></li>
<li><a href="#12-visualizing-convolution">1.2 Visualizing Convolution</a></li>
</ul>
</li>
<li>
<p><a href="#2-diffusion-model-principles">2. Diffusion Model Principles</a></p>
<ul>
<li><a href="#21-principles-of-early-generative-models">2.1 Principles of Early Generative Models</a></li>
<li><a href="#22-diffusion-model">2.2 Diffusion Model</a></li>
<li><a href="#23-convolution-in-diffusion-models">2.3 Convolution in Diffusion Models</a></li>
</ul>
</li>
<li>
<p><a href="#3-code-implementation-of-diffusion">3. Code Implementation of Diffusion</a></p>
<ul>
<li><a href="#31-importing-required-libraries">3.1 Importing Required Libraries</a></li>
<li><a href="#32-using-the-mnist-dataset">3.2 Using the MNIST Dataset</a></li>
<li><a href="#33-writing-the-noise-corruption-formula">3.3 Writing the Noise Corruption Formula</a></li>
<li><a href="#34-creating-a-simple-unet-model">3.4 Creating a Simple UNet Model</a></li>
<li><a href="#35-defining-the-training-parameters">3.5 Defining the Training Parameters</a></li>
<li><a href="#36-testing-the-model">3.6 Testing the Model</a></li>
</ul>
</li>
<li>
<p><a href="#Conclusion">Conclusion</a></p>
</li>
</ul>
<h1>1. What is Convolution?</h1>
<h2>1.1 Understanding Convolution from a Mathematical Perspective</h2>
<p>The mathematical formula for convolution is typically represented as the convolution of two functions, ( f(x) ) and ( g(x) ). It is defined as:</p>
<p>$$
(f * g)(x) = \int_{-\infty}^{\infty} f(t) \cdot g(x - t) , dt
$$</p>
<p>Where:</p>
<ul>
<li>( f(x) ) and ( g(x) ) are the two functions to be convolved.</li>
<li>( (f * g)(x) ) is the resulting function after the convolution.</li>
<li>( t ) is the integration variable.</li>
</ul>
<p>For discrete convolution, the formula is:</p>
<p>$$
(f * g)[n] = \sum_{k=-\infty}^{\infty} f[k] \cdot g[n - k]
$$</p>
<p>Here, ( f[k] ) and ( g[k] ) are discrete signals, and ( n ) is the discrete output index.</p>
<h2>1.2 Visualizing Convolution</h2>
<p>In the image, the left part shows the pixel value matrix of a grayscale image (i.e., how the image is represented as numbers for a computer). In the middle is the <strong>convolution kernel</strong> matrix, which slides over the original image starting from the top-left corner. The kernel computes a value at each position, and this process is repeated across the image. The resulting values form the <strong>right image (feature map)</strong>, which contains the local features of the original image obtained through the convolution process.</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9a38553f8c3812bc57e4357dadf3afcde48e1034492c117d111ecbb7f40fbc31/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f61383232383230623330336434656166613032336430326139373839383830372e706e67"><img src="https://camo.githubusercontent.com/9a38553f8c3812bc57e4357dadf3afcde48e1034492c117d111ecbb7f40fbc31/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f61383232383230623330336434656166613032336430326139373839383830372e706e67" alt="Image here" data-canonical-src="https://i-blog.csdnimg.cn/direct/a822820b303d4eafa023d02a97898807.png" style="max-width: 100%;"></a></p>
<p>The animation works like this: <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bcc8ccdbe4ed644c9461ee46ef4987560a8300d7abdb87096a219b5a2edf821e/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f65386639316333303563636334626536396135323836323336333534356466322e77656270"><img src="https://camo.githubusercontent.com/bcc8ccdbe4ed644c9461ee46ef4987560a8300d7abdb87096a219b5a2edf821e/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f65386639316333303563636334626536396135323836323336333534356466322e77656270" alt="Image here" data-canonical-src="https://i-blog.csdnimg.cn/direct/e8f91c305ccc4be69a52862363545df2.webp" style="max-width: 100%;"></a></p>
<h1>2. Diffusion Model Principles</h1>
<h2>2.1 Principles of Early Generative Models</h2>
<p>Early generative models such as GAN (Generative Adversarial Networks) and VAE (Variational Autoencoders) involve inverting the original model. For instance, in GAN, the recognition model is a conventional convolution network used for identifying generated images. The generation model, however, reverses this by using a transposed convolution network (also called deconvolution) to generate images, but this approach doesn't produce ideal results.</p>
<p>Let's talk about transposed convolution. Transposed convolution is the reverse of convolution: convolution turns a large matrix into a smaller one, whereas transposed convolution takes a smaller matrix and generates a larger one. As shown in the image below, it creates the dashed-line matrix! <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5ff8efe89d43dc0249861326a2af3cc1d9613c9fc72ef153df076948b9176d3c/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f61373162343461643434343334666437386136623332653233393330363562372e676966"><img src="https://camo.githubusercontent.com/5ff8efe89d43dc0249861326a2af3cc1d9613c9fc72ef153df076948b9176d3c/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f61373162343461643434343334666437386136623332653233393330363562372e676966" alt="" data-canonical-src="https://i-blog.csdnimg.cn/direct/a71b44ad44434fd78a6b32e2393065b7.gif" style="max-width: 100%;"></a></p>
<h2>2.2 Diffusion Model</h2>
<p>While directly generating images is not ideal, scientists have drawn inspiration from diffusion in physics. In nature, substances tend to move toward a disordered state. For example, when a drop of ink is added to a glass of water, it gradually spreads out. This suggests that generative models could also take a gradual, step-by-step approach instead of rushing, aiming for steady progress.</p>
<p>Thus, diffusion models were born. We start by adding noise to an image’s pixels, which results in a very chaotic image. Conversely, we can also reverse this process to recover the original image from this noisy one.</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9dba46bd3de00f0cbd4f4d2f53a870a9f3af43d2ce4552340bd87af7ffa6e82d/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f36633164666534396463353934316337393763306138383638626535623739622e706e67"><img src="https://camo.githubusercontent.com/9dba46bd3de00f0cbd4f4d2f53a870a9f3af43d2ce4552340bd87af7ffa6e82d/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f36633164666534396463353934316337393763306138383638626535623739622e706e67" alt="Image here" data-canonical-src="https://i-blog.csdnimg.cn/direct/6c1dfe49dc5941c797c0a8868be5b79b.png" style="max-width: 100%;"></a></p>
<h2>2.3 Convolution in Diffusion Models</h2>
<p>Diffusion models typically use a UNet network to predict denoised images, with the addition of <strong>timestep</strong> to reflect the noise level. The prediction is done for each <strong>timestep</strong> of the image.</p>
<p>As shown in the image, this is a convolution kernel from the UNet network used in diffusion (with code implementation to follow). In fact, throughout the entire network, the properties of the convolution kernel remain largely unchanged, and the input's width and height do not change during the forward pass. Only the number of channels changes.</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/735787202358be073757e76c9633a93de4fcf9bc21d01eed0718d20b483ee7e8/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f37363338393435366232613834333662386433383135366532326237333236662e706e67"><img src="https://camo.githubusercontent.com/735787202358be073757e76c9633a93de4fcf9bc21d01eed0718d20b483ee7e8/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f37363338393435366232613834333662386433383135366532326237333236662e706e67" alt="Image here" data-canonical-src="https://i-blog.csdnimg.cn/direct/76389456b2a8436b8d38156e22b7326f.png" style="max-width: 100%;"></a><br>
We remembered, <strong>convolution</strong> maps a matrix onto a feature matrix, while <strong>diffusion</strong> introduces disorder into the matrix.Try to think of it this way: <strong>convolution</strong> disturbs or restores the local features of a matrix, while <strong>diffusion</strong> relies on <strong>convolution</strong> to diffuse local features.</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/06e0f08dde62c0e9e0f603a661b76289271e2096cb2bbe9aa0976bc77de0c904/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f31303636623966346664386334663366623730633335306536363739633632342e706e67"><img src="https://camo.githubusercontent.com/06e0f08dde62c0e9e0f603a661b76289271e2096cb2bbe9aa0976bc77de0c904/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f31303636623966346664386334663366623730633335306536363739633632342e706e67" alt="Image here" data-canonical-src="https://i-blog.csdnimg.cn/direct/1066b9f4fd8c4f3fb70c350e6679c624.png" style="max-width: 100%;"></a></p>
<h1>3. Code Implementation of Diffusion</h1>
<p>Theory is one thing, but let’s dive into a practical example.</p>
<h2>3.1 Importing Required Libraries</h2>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">import</span> <span class="pl-s1">torchvision</span>
<span class="pl-k">from</span> <span class="pl-s1">torch</span> <span class="pl-k">import</span> <span class="pl-s1">nn</span>
<span class="pl-k">from</span> <span class="pl-s1">torch</span>.<span class="pl-s1">nn</span> <span class="pl-k">import</span> <span class="pl-s1">functional</span> <span class="pl-k">as</span> <span class="pl-c1">F</span>
<span class="pl-k">from</span> <span class="pl-s1">torch</span>.<span class="pl-s1">utils</span>.<span class="pl-s1">data</span> <span class="pl-k">import</span> <span class="pl-v">DataLoader</span>
<span class="pl-k">from</span> <span class="pl-s1">diffusers</span> <span class="pl-k">import</span> <span class="pl-v">DDPMScheduler</span>, <span class="pl-v">UNet2DModel</span>
<span class="pl-k">from</span> <span class="pl-s1">matplotlib</span> <span class="pl-k">import</span> <span class="pl-s1">pyplot</span> <span class="pl-k">as</span> <span class="pl-s1">plt</span>
<span class="pl-k">import</span> <span class="pl-s1">os</span>

<span class="pl-s1">device</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-c1">device</span>(<span class="pl-s">"cuda"</span> <span class="pl-k">if</span> <span class="pl-s1">torch</span>.<span class="pl-c1">cuda</span>.<span class="pl-c1">is_available</span>() <span class="pl-k">else</span> <span class="pl-s">"cpu"</span>)
<span class="pl-en">print</span>(<span class="pl-s">f"Using device: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">device</span><span class="pl-kos">}</span></span>"</span>)</pre></div>
<h2>3.2 Using the MNIST Dataset</h2>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-s1">dataset</span> <span class="pl-c1">=</span> <span class="pl-s1">torchvision</span>.<span class="pl-c1">datasets</span>.<span class="pl-c1">MNIST</span>(
    <span class="pl-s1">root</span><span class="pl-c1">=</span><span class="pl-s">"./data"</span>, <span class="pl-s1">train</span><span class="pl-c1">=</span><span class="pl-c1">True</span>, <span class="pl-s1">download</span><span class="pl-c1">=</span><span class="pl-c1">True</span>, <span class="pl-s1">transform</span><span class="pl-c1">=</span><span class="pl-s1">torchvision</span>.<span class="pl-c1">transforms</span>.<span class="pl-c1">ToTensor</span>()
)
<span class="pl-s1">train_dataloader</span> <span class="pl-c1">=</span> <span class="pl-en">DataLoader</span>(<span class="pl-s1">dataset</span>, <span class="pl-s1">batch_size</span><span class="pl-c1">=</span><span class="pl-c1">8</span>, <span class="pl-s1">shuffle</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)</pre></div>
<h2>3.3 Writing the Noise Corruption Formula</h2>
<p>Corrupting means mixing the image with noise in a certain proportion to achieve denoising. As the diffusion process progresses, the image becomes clearer, and the noise has less of an effect.</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">def</span> <span class="pl-en">corrupt</span>(<span class="pl-s1">x</span>, <span class="pl-s1">amount</span>):
    <span class="pl-s">"""Corrupt the input `x` by mixing it with noise according to `amount`"""</span>
    <span class="pl-s1">noise</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-c1">rand_like</span>(<span class="pl-s1">x</span>)
    <span class="pl-s1">amount</span> <span class="pl-c1">=</span> <span class="pl-s1">amount</span>.<span class="pl-c1">view</span>(<span class="pl-c1">-</span><span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>)  <span class="pl-c"># Adjust shape for broadcasting</span>
    <span class="pl-k">return</span> <span class="pl-s1">x</span> <span class="pl-c1">*</span> (<span class="pl-c1">1</span> <span class="pl-c1">-</span> <span class="pl-s1">amount</span>) <span class="pl-c1">+</span> <span class="pl-s1">noise</span> <span class="pl-c1">*</span> <span class="pl-s1">amount</span></pre></div>
<h2>3.4 Creating a Simple UNet Model</h2>
<p>We’ll use a mini UNet model (not the standard one) that still achieves good results.</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">class</span> <span class="pl-v">BasicUNet</span>(<span class="pl-s1">nn</span>.<span class="pl-c1">Module</span>):
    <span class="pl-s">"""A minimal UNet implementation."""</span>

    <span class="pl-k">def</span> <span class="pl-en">__init__</span>(<span class="pl-s1">self</span>, <span class="pl-s1">in_channels</span><span class="pl-c1">=</span><span class="pl-c1">1</span>, <span class="pl-s1">out_channels</span><span class="pl-c1">=</span><span class="pl-c1">1</span>):
        <span class="pl-en">super</span>().<span class="pl-c1">__init__</span>()
        <span class="pl-s1">self</span>.<span class="pl-c1">down_layers</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-c1">nn</span>.<span class="pl-c1">ModuleList</span>(
            [
                <span class="pl-s1">nn</span>.<span class="pl-c1">Conv2d</span>(<span class="pl-s1">in_channels</span>, <span class="pl-c1">32</span>, <span class="pl-s1">kernel_size</span><span class="pl-c1">=</span><span class="pl-c1">5</span>, <span class="pl-s1">padding</span><span class="pl-c1">=</span><span class="pl-c1">2</span>),
                <span class="pl-s1">nn</span>.<span class="pl-c1">Conv2d</span>(<span class="pl-c1">32</span>, <span class="pl-c1">64</span>, <span class="pl-s1">kernel_size</span><span class="pl-c1">=</span><span class="pl-c1">5</span>, <span class="pl-s1">padding</span><span class="pl-c1">=</span><span class="pl-c1">2</span>),
                <span class="pl-s1">nn</span>.<span class="pl-c1">Conv2d</span>(<span class="pl-c1">64</span>, <span class="pl-c1">64</span>, <span class="pl-s1">kernel_size</span><span class="pl-c1">=</span><span class="pl-c1">5</span>, <span class="pl-s1">padding</span><span class="pl-c1">=</span><span class="pl-c1">2</span>),
            ]
        )
        <span class="pl-s1">self</span>.<span class="pl-c1">up_layers</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-c1">nn</span>.<span class="pl-c1">ModuleList</span>(
            [
                <span class="pl-s1">nn</span>.<span class="pl-c1">Conv2d</span>(<span class="pl-c1">64</span>, <span class="pl-c1">64</span>, <span class="pl-s1">kernel_size</span><span class="pl-c1">=</span><span class="pl-c1">5</span>, <span class="pl-s1">padding</span><span class="pl-c1">=</span><span class="pl-c1">2</span>),
                <span class="pl-s1">nn</span>.<span class="pl-c1">Conv2d</span>(<span class="pl-c1">64</span>, <span class="pl-c1">32</span>, <span class="pl-s1">kernel_size</span><span class="pl-c1">=</span><span class="pl-c1">5</span>, <span class="pl-s1">padding</span><span class="pl-c1">=</span><span class="pl-c1">2</span>),
                <span class="pl-s1">nn</span>.<span class="pl-c1">Conv2d</span>(<span class="pl-c1">32</span>, <span class="pl-s1">out_channels</span>, <span class="pl-s1">kernel_size</span><span class="pl-c1">=</span><span class="pl-c1">5</span>, <span class="pl-s1">padding</span><span class="pl-c1">=</span><span class="pl-c1">2</span>),
            ]
        )
        <span class="pl-s1">self</span>.<span class="pl-c1">act</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-c1">SiLU</span>()  <span class="pl-c"># Activation function</span>
        <span class="pl-s1">self</span>.<span class="pl-c1">downscale</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-c1">MaxPool2d</span>(<span class="pl-c1">2</span>)
        <span class="pl-s1">self</span>.<span class="pl-c1">upscale</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-c1">Upsample</span>(<span class="pl-s1">scale_factor</span><span class="pl-c1">=</span><span class="pl-c1">2</span>)

    <span class="pl-k">def</span> <span class="pl-en">forward</span>(<span class="pl-s1">self</span>, <span class="pl-s1">x</span>):
        <span class="pl-s1">h</span> <span class="pl-c1">=</span> []
        <span class="pl-k">for</span> <span class="pl-s1">i</span>, <span class="pl-s1">l</span> <span class="pl-c1">in</span> <span class="pl-en">enumerate</span>(<span class="pl-s1">self</span>.<span class="pl-c1">down_layers</span>):
            <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-c1">act</span>(<span class="pl-en">l</span>(<span class="pl-s1">x</span>))  <span class="pl-c"># Pass through the layer and activation function</span>
            <span class="pl-k">if</span> <span class="pl-s1">i</span> <span class="pl-c1">&lt;</span> <span class="pl-c1">2</span>:  <span class="pl-c"># Skip connection for all but the final down layer</span>
                <span class="pl-s1">h</span>.<span class="pl-c1">append</span>(<span class="pl-s1">x</span>)  <span class="pl-c"># Store output for skip connection</span>
                <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-c1">downscale</span>(<span class="pl-s1">x</span>)  <span class="pl-c"># Downscale for next layer</span>

        <span class="pl-k">for</span> <span class="pl-s1">i</span>, <span class="pl-s1">l</span> <span class="pl-c1">in</span> <span class="pl-en">enumerate</span>(<span class="pl-s1">self</span>.<span class="pl-c1">up_layers</span>):
            <span class="pl-k">if</span> <span class="pl-s1">i</span> <span class="pl-c1">&gt;</span> <span class="pl-c1">0</span>:  <span class="pl-c"># For all but the first up layer</span>
                <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-c1">upscale</span>(<span class="pl-s1">x</span>)  <span class="pl-c"># Upscale</span>
                <span class="pl-s1">x</span> <span class="pl-c1">+=</span> <span class="pl-s1">h</span>.<span class="pl-c1">pop</span>()  <span class="pl-c"># Use stored output (skip connection)</span>
            <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-c1">act</span>(<span class="pl-en">l</span>(<span class="pl-s1">x</span>))  <span class="pl-c"># Pass through the layer and activation function</span>

        <span class="pl-k">return</span> <span class="pl-s1">x</span></pre></div>
<p>The network looks like this:</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8989f318be800b9ddb59ca712e60b3c349f4a5a08c8ef33ae71be6a2635442d7/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f39643838353032336536376434336464383463376535623237626437373063392e706e67"><img src="https://camo.githubusercontent.com/8989f318be800b9ddb59ca712e60b3c349f4a5a08c8ef33ae71be6a2635442d7/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f39643838353032336536376434336464383463376535623237626437373063392e706e67" alt="Image here" data-canonical-src="https://i-blog.csdnimg.cn/direct/9d885023e67d43dd84c7e5b27bd770c9.png" style="max-width: 100%;"></a></p>
<h2>3.5 Defining the Training Parameters</h2>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># Dataloader (adjust batch size)</span>
<span class="pl-s1">batch_size</span> <span class="pl-c1">=</span> <span class="pl-c1">128</span>
<span class="pl-s1">train_dataloader</span> <span class="pl-c1">=</span> <span class="pl-en">DataLoader</span>(<span class="pl-s1">dataset</span>, <span class="pl-s1">batch_size</span><span class="pl-c1">=</span><span class="pl-s1">batch_size</span>, <span class="pl-s1">shuffle</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)

<span class="pl-c"># Number of epochs</span>
<span class="pl-s1">n_epochs</span> <span class="pl-c1">=</span> <span class="pl-c1">30</span>

<span class="pl-c"># Create the network</span>
<span class="pl-s1">net</span> <span class="pl-c1">=</span> <span class="pl-en">UNet2DModel</span>(
    <span class="pl-s1">sample_size</span><span class="pl-c1">=</span><span class="pl-c1">28</span>,  <span class="pl-c"># Target image resolution</span>
    <span class="pl-s1">in_channels</span><span class="pl-c1">=</span><span class="pl-c1">1</span>,  <span class="pl-c"># Input channels, 3 for RGB images</span>
    <span class="pl-s1">out_channels</span><span class="pl-c1">=</span><span class="pl-c1">1</span>,  <span class="pl-c"># Output channels</span>
    <span class="pl-s1">layers_per_block</span><span class="pl-c1">=</span><span class="pl-c1">2</span>,  <span class="pl-c"># ResNet layers per UNet block</span>
    <span class="pl-s1">block_out_channels</span><span class="pl-c1">=</span>(<span class="pl-c1">32</span>, <span class="pl-c1">64</span>, <span class="pl-c1">64</span>),  <span class="pl-c"># Matching our basic UNet example</span>
    <span class="pl-s1">down_block_types</span><span class="pl-c1">=</span>(
        <span class="pl-s">"DownBlock2D"</span>,  <span class="pl-c"># Regular ResNet downsampling block</span>
        <span class="pl-s">"AttnDownBlock2D"</span>,  <span class="pl-c"># ResNet block with attention</span>
        <span class="pl-s">"AttnDownBlock2D"</span>,
    ),
    <span class="pl-s1">up_block_types</span><span class="pl-c1">=</span>(
        <span class="pl-s">"AttnUpBlock2D"</span>,
        <span class="pl-s">"AttnUpBlock2D"</span>,  <span class="pl-c"># ResNet block with attention</span>
        <span class="pl-s">"UpBlock2D"</span>,  <span class="pl-c"># Regular ResNet upsampling block</span>
    ),
)
<span class="pl-s1">net</span>.<span class="pl-c1">to</span>(<span class="pl-s1">device</span>)

<span class="pl-c"># Loss function</span>
<span class="pl-s1">loss_fn</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-c1">MSELoss</span>()

<span class="pl-c"># Optimizer</span>
<span class="pl-s1">opt</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-c1">optim</span>.<span class="pl-c1">Adam</span>(<span class="pl-s1">net</span>.<span class="pl-c1">parameters</span>(), <span class="pl-s1">lr</span><span class="pl-c1">=</span><span class="pl-c1">1e-3</span>)

<span class="pl-c"># Track losses</span>
<span class="pl-s1">losses</span> <span class="pl-c1">=</span> []

<span class="pl-c"># Training loop</span>
<span class="pl-k">for</span> <span class="pl-s1">epoch</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-s1">n_epochs</span>):
    <span class="pl-k">for</span> <span class="pl-s1">x</span>, <span class="pl-s1">y</span> <span class="pl-c1">in</span> <span class="pl-s1">train_dataloader</span>:
        <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">x</span>.<span class="pl-c1">to</span>(<span class="pl-s1">device</span>)  <span class="pl-c"># Data on GPU</span>
        <span class="pl-s1">noise_amount</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-c1">rand</span>(<span class="pl-s1">x</span>.<span class="pl-c1">shape</span>[<span class="pl-c1">0</span>]).<span class="pl-c1">to</span>(<span class="pl-s1">device</span>)  <span class="pl-c"># Random noise amount</span>
        <span class="pl-s1">noisy_x</span> <span class="pl-c1">=</span> <span class="pl-en">corrupt</span>(<span class="pl-s1">x</span>, <span class="pl-s1">noise_amount</span>)  <span class="pl-c"># Create noisy input</span>

        <span class="pl-c"># Get model prediction</span>
        <span class="pl-s1">pred</span> <span class="pl-c1">=</span> <span class="pl-en">net</span>(<span class="pl-s1">noisy_x</span>, <span class="pl-c1">0</span>).<span class="pl-c1">sample</span>  <span class="pl-c"># Use timestep 0</span>

        <span class="pl-c"># Calculate the loss</span>
        <span class="pl-s1">loss</span> <span class="pl-c1">=</span> <span class="pl-en">loss_fn</span>(<span class="pl-s1">pred</span>, <span class="pl-s1">x</span>)  <span class="pl-c"># Compare to original clean image</span>

        <span class="pl-c"># Backprop and update parameters</span>
        <span class="pl-s1">opt</span>.<span class="pl-c1">zero_grad</span>()
        <span class="pl-s1">loss</span>.<span class="pl-c1">backward</span>()
        <span class="pl-s1">opt</span>.<span class="pl-c1">step</span>()

        <span class="pl-s1">losses</span>.<span class="pl-c1">append</span>(<span class="pl-s1">loss</span>.<span class="pl-c1">item</span>())

    <span class="pl-s1">avg_loss</span> <span class="pl-c1">=</span> <span class="pl-en">sum</span>(<span class="pl-s1">losses</span>[<span class="pl-c1">-</span><span class="pl-en">len</span>(<span class="pl-s1">train_dataloader</span>):]) <span class="pl-c1">/</span> <span class="pl-en">len</span>(<span class="pl-s1">train_dataloader</span>)
    <span class="pl-en">print</span>(<span class="pl-s">f"Epoch <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">epoch</span><span class="pl-kos">}</span></span>. Average loss: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">avg_loss</span>:.5f<span class="pl-kos">}</span></span>"</span>)</pre></div>
<h2>3.6 Testing the Model</h2>
<p>After training, we can test the diffusion model’s power:</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-s1">n_steps</span> <span class="pl-c1">=</span> <span class="pl-c1">40</span>
<span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-c1">rand</span>(<span class="pl-c1">8</span>, <span class="pl-c1">1</span>, <span class="pl-c1">28</span>, <span class="pl-c1">28</span>).<span class="pl-c1">to</span>(<span class="pl-s1">device</span>)  <span class="pl-c"># Start from random noise</span>
<span class="pl-s1">step_history</span> <span class="pl-c1">=</span> [<span class="pl-s1">x</span>.<span class="pl-c1">detach</span>().<span class="pl-c1">cpu</span>()]
<span class="pl-s1">pred_output_history</span> <span class="pl-c1">=</span> []

<span class="pl-k">for</span> <span class="pl-s1">i</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-s1">n_steps</span>):
    <span class="pl-k">with</span> <span class="pl-s1">torch</span>.<span class="pl-c1">no_grad</span>():  <span class="pl-c"># No gradients during inference</span>
        <span class="pl-s1">pred</span> <span class="pl-c1">=</span> <span class="pl-en">net</span>(<span class="pl-s1">x</span>,<span class="pl-c1">0</span>).<span class="pl-c1">sample</span>  <span class="pl-c"># Predict denoised image</span>
    <span class="pl-s1">pred_output_history</span>.<span class="pl-c1">append</span>(<span class="pl-s1">pred</span>.<span class="pl-c1">detach</span>().<span class="pl-c1">cpu</span>())  <span class="pl-c"># Store prediction</span>
    <span class="pl-s1">mix_factor</span> <span class="pl-c1">=</span> <span class="pl-c1">1</span> <span class="pl-c1">/</span> (<span class="pl-s1">n_steps</span> <span class="pl-c1">-</span> <span class="pl-s1">i</span>)  <span class="pl-c"># Mix towards prediction</span>
    <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">x</span> <span class="pl-c1">*</span> (<span class="pl-c1">1</span> <span class="pl-c1">-</span> <span class="pl-s1">mix_factor</span>) <span class="pl-c1">+</span> <span class="pl-s1">pred</span> <span class="pl-c1">*</span> <span class="pl-s1">mix_factor</span>  <span class="pl-c"># Move partway to the denoised image</span>
    <span class="pl-s1">step_history</span>.<span class="pl-c1">append</span>(<span class="pl-s1">x</span>.<span class="pl-c1">detach</span>().<span class="pl-c1">cpu</span>())  <span class="pl-c"># Store for plotting</span>

<span class="pl-s1">fig</span>, <span class="pl-s1">axs</span> <span class="pl-c1">=</span> <span class="pl-s1">plt</span>.<span class="pl-c1">subplots</span>(<span class="pl-s1">n_steps</span>, <span class="pl-c1">2</span>, <span class="pl-s1">figsize</span><span class="pl-c1">=</span>(<span class="pl-c1">9</span>, <span class="pl-c1">32</span>), <span class="pl-s1">sharex</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-s1">axs</span>[<span class="pl-c1">0</span>, <span class="pl-c1">0</span>].<span class="pl-c1">set_title</span>(<span class="pl-s">"Input (noisy)"</span>)
<span class="pl-s1">axs</span>[<span class="pl-c1">0</span>, <span class="pl-c1">1</span>].<span class="pl-c1">set_title</span>(<span class="pl-s">"Model Prediction"</span>)
<span class="pl-k">for</span> <span class="pl-s1">i</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-s1">n_steps</span>):
    <span class="pl-s1">axs</span>[<span class="pl-s1">i</span>, <span class="pl-c1">0</span>].<span class="pl-c1">imshow</span>(<span class="pl-s1">torchvision</span>.<span class="pl-c1">utils</span>.<span class="pl-c1">make_grid</span>(<span class="pl-s1">step_history</span>[<span class="pl-s1">i</span>])[<span class="pl-c1">0</span>].<span class="pl-c1">clip</span>(<span class="pl-c1">0</span>, <span class="pl-c1">1</span>), <span class="pl-s1">cmap</span><span class="pl-c1">=</span><span class="pl-s">"Greys"</span>)
    <span class="pl-s1">axs</span>[<span class="pl-s1">i</span>, <span class="pl-c1">1</span>].<span class="pl-c1">imshow</span>(<span class="pl-s1">torchvision</span>.<span class="pl-c1">utils</span>.<span class="pl-c1">make_grid</span>(<span class="pl-s1">pred_output_history</span>[<span class="pl-s1">i</span>])[<span class="pl-c1">0</span>].<span class="pl-c1">clip</span>(<span class="pl-c1">0</span>, <span class="pl-c1">1</span>), <span class="pl-s1">cmap</span><span class="pl-c1">=</span><span class="pl-s">"Greys"</span>)</pre></div>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6cd7f76fbdd88bfc94398be94d7cd68f5d79c535451a28fa6d717d037bb7e30e/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f38346661393236666465343134366136383132303533623564383866333636622e706e67"><img src="https://camo.githubusercontent.com/6cd7f76fbdd88bfc94398be94d7cd68f5d79c535451a28fa6d717d037bb7e30e/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f6469726563742f38346661393236666465343134366136383132303533623564383866333636622e706e67" alt="Image here" data-canonical-src="https://i-blog.csdnimg.cn/direct/84fa926fde4146a6812053b5d88f366b.png" style="max-width: 100%;"></a></p>
<p>This shows the final results of testing the model, and the output looks quite impressive!</p>
<h1>Conclusion</h1>
<p>This concludes the explanation of the diffusion model. You can try modifying the UNet network or adjusting parameters to see if you can achieve even more remarkable results!</p></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://Refound-445.github.io">My Blogs</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z', 'copy': 'M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z', 'check': 'M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","Refound-445/Refound-445.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}

document.addEventListener('DOMContentLoaded', () => {
    const createClipboardHTML = (codeContent, additionalClasses = '') => `
        <pre class="notranslate"><code class="notranslate">${codeContent}</code></pre>
        <div class="clipboard-container position-absolute right-0 top-0 ${additionalClasses}">
            <clipboard-copy class="ClipboardButton btn m-2 p-0" role="button" style="display: inherit;">
                <svg height="16" width="16" class="octicon octicon-copy m-2"><path d="${IconList["copy"]}"></path></svg>
                <svg height="16" width="16" class="octicon octicon-check color-fg-success m-2 d-none"><path d="${IconList["check"]}"></path></svg>
            </clipboard-copy>
            <div class="copy-feedback">Copied!</div>
        </div>
    `;

    const handleCodeElements = (selector = '') => {
        document.querySelectorAll(selector).forEach(codeElement => {
            const codeContent = codeElement.innerHTML;
            const newStructure = document.createElement('div');
            newStructure.className = 'snippet-clipboard-content position-relative overflow-auto';
            newStructure.innerHTML = createClipboardHTML(codeContent);

            const parentElement = codeElement.parentElement;
            if (selector.includes('highlight')) {
                parentElement.insertBefore(newStructure, codeElement.nextSibling);
                parentElement.removeChild(codeElement);
            } else {
                parentElement.parentElement.replaceChild(newStructure, parentElement);
            }
        });
    };

    handleCodeElements('pre.notranslate > code.notranslate');
    handleCodeElements('div.highlight > pre.notranslate');

    let currentFeedback = null;
    document.querySelectorAll('clipboard-copy').forEach(copyButton => {
        copyButton.addEventListener('click', () => {
            const codeContent = copyButton.closest('.snippet-clipboard-content').innerText;
            const tempTextArea = document.createElement('textarea');
            tempTextArea.value = codeContent;
            document.body.appendChild(tempTextArea);
            tempTextArea.select();
            document.execCommand('copy');
            document.body.removeChild(tempTextArea);

            const copyIcon = copyButton.querySelector('.octicon-copy');
            const checkIcon = copyButton.querySelector('.octicon-check');
            const copyFeedback = copyButton.nextElementSibling;

            if (currentFeedback && currentFeedback !== copyFeedback) {currentFeedback.style.display = 'none';}
            currentFeedback = copyFeedback;

            copyIcon.classList.add('d-none');
            checkIcon.classList.remove('d-none');
            copyFeedback.style.display = 'block';
            copyButton.style.borderColor = 'var(--color-success-fg)';

            setTimeout(() => {
                copyIcon.classList.remove('d-none');
                checkIcon.classList.add('d-none');
                copyFeedback.style.display = 'none';
                copyButton.style.borderColor = '';
            }, 2000);
        });
    });
});

</script>
<script>MathJax = {tex: {inlineMath: [["$", "$"]]}};</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</html>
