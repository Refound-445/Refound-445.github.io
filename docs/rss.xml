<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>My Blogs</title><link>https://Refound-445.github.io</link><description>Just My Blogs</description><copyright>My Blogs</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/137168144</url><title>avatar</title><link>https://Refound-445.github.io</link></image><lastBuildDate>Wed, 15 Jan 2025 18:26:55 +0000</lastBuildDate><managingEditor>My Blogs</managingEditor><ttl>60</ttl><webMaster>My Blogs</webMaster><item><title>The Diffusion Principle Understood From The Perspective Of Convolution</title><link>https://Refound-445.github.io/post/The%20Diffusion%20Principle%20Understood%20From%20The%20Perspective%20Of%20Convolution.html</link><description># 1. What is Convolution?&#13;
&#13;
## 1.1 Understanding Convolution from a Mathematical Perspective&#13;
&#13;
The mathematical formula for convolution is typically represented as the convolution of two functions, \( f(x) \) and \( g(x) \). It is defined as:&#13;
&#13;
$$&#13;
(f * g)(x) = \int_{-\infty}^{\infty} f(t) \cdot g(x - t) \, dt&#13;
$$&#13;
&#13;
Where:&#13;
&#13;
- \( f(x) \) and \( g(x) \) are the two functions to be convolved.&#13;
- \( (f * g)(x) \) is the resulting function after the convolution.&#13;
- \( t \) is the integration variable.&#13;
&#13;
For discrete convolution, the formula is:&#13;
&#13;
$$&#13;
(f * g)[n] = \sum_{k=-\infty}^{\infty} f[k] \cdot g[n - k]&#13;
$$&#13;
&#13;
Here, \( f[k] \) and \( g[k] \) are discrete signals, and \( n \) is the discrete output index.&#13;
&#13;
## 1.2 Visualizing Convolution&#13;
&#13;
In the image, the left part shows the pixel value matrix of a grayscale image (i.e., how the image is represented as numbers for a computer). In the middle is the **convolution kernel** matrix, which slides over the original image starting from the top-left corner. The kernel computes a value at each position, and this process is repeated across the image. The resulting values form the **right image (feature map)**, which contains the local features of the original image obtained through the convolution process.&#13;
&#13;
![Image here](https://i-blog.csdnimg.cn/direct/a822820b303d4eafa023d02a97898807.png)&#13;
&#13;
The animation works like this: ![Image here](https://i-blog.csdnimg.cn/direct/e8f91c305ccc4be69a52862363545df2.webp)&#13;
&#13;
# 2. Diffusion Model Principles&#13;
&#13;
## 2.1 Principles of Early Generative Models&#13;
&#13;
Early generative models such as GAN (Generative Adversarial Networks) and VAE (Variational Autoencoders) involve inverting the original model. For instance, in GAN, the recognition model is a conventional convolution network used for identifying generated images. The generation model, however, reverses this by using a transposed convolution network (also called deconvolution) to generate images, but this approach doesn't produce ideal results.&#13;
&#13;
Let's talk about transposed convolution. Transposed convolution is the reverse of convolution: convolution turns a large matrix into a smaller one, whereas transposed convolution takes a smaller matrix and generates a larger one. As shown in the image below, it creates the dashed-line matrix! ![](https://i-blog.csdnimg.cn/direct/a71b44ad44434fd78a6b32e2393065b7.gif)&#13;
&#13;
## 2.2 Diffusion Model&#13;
&#13;
While directly generating images is not ideal, scientists have drawn inspiration from diffusion in physics. In nature, substances tend to move toward a disordered state. For example, when a drop of ink is added to a glass of water, it gradually spreads out. This suggests that generative models could also take a gradual, step-by-step approach instead of rushing, aiming for steady progress.&#13;
&#13;
Thus, diffusion models were born. We start by adding noise to an image’s pixels, which results in a very chaotic image. Conversely, we can also reverse this process to recover the original image from this noisy one.&#13;
&#13;
![Image here](https://i-blog.csdnimg.cn/direct/6c1dfe49dc5941c797c0a8868be5b79b.png)&#13;
&#13;
## 2.3 Convolution in Diffusion Models&#13;
&#13;
Diffusion models typically use a UNet network to predict denoised images, with the addition of **timestep** to reflect the noise level. The prediction is done for each **timestep** of the image.&#13;
&#13;
As shown in the image, this is a convolution kernel from the UNet network used in diffusion (with code implementation to follow). In fact, throughout the entire network, the properties of the convolution kernel remain largely unchanged, and the input's width and height do not change during the forward pass. Only the number of channels changes.&#13;
&#13;
![Image here](https://i-blog.csdnimg.cn/direct/76389456b2a8436b8d38156e22b7326f.png)&#13;
We remembered, **convolution** maps a matrix onto a feature matrix, while **diffusion** introduces disorder into the matrix.Try to think of it this way: **convolution** disturbs or restores the local features of a matrix, while **diffusion** relies on **convolution** to diffuse local features.&#13;
&#13;
![Image here](https://i-blog.csdnimg.cn/direct/1066b9f4fd8c4f3fb70c350e6679c624.png)&#13;
&#13;
# 3. Code Implementation of Diffusion&#13;
&#13;
Theory is one thing, but let’s dive into a practical example.&#13;
&#13;
## 3.1 Importing Required Libraries&#13;
&#13;
```python&#13;
import torch&#13;
import torchvision&#13;
from torch import nn&#13;
from torch.nn import functional as F&#13;
from torch.utils.data import DataLoader&#13;
from diffusers import DDPMScheduler, UNet2DModel&#13;
from matplotlib import pyplot as plt&#13;
import os&#13;
&#13;
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')&#13;
print(f'Using device: {device}')&#13;
```&#13;
&#13;
## 3.2 Using the MNIST Dataset&#13;
&#13;
```python&#13;
dataset = torchvision.datasets.MNIST(&#13;
    root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor()&#13;
)&#13;
train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)&#13;
```&#13;
&#13;
## 3.3 Writing the Noise Corruption Formula&#13;
&#13;
Corrupting means mixing the image with noise in a certain proportion to achieve denoising. As the diffusion process progresses, the image becomes clearer, and the noise has less of an effect.&#13;
&#13;
```python&#13;
def corrupt(x, amount):&#13;
    '''Corrupt the input `x` by mixing it with noise according to `amount`'''&#13;
    noise = torch.rand_like(x)&#13;
    amount = amount.view(-1, 1, 1, 1)  # Adjust shape for broadcasting&#13;
    return x * (1 - amount) + noise * amount&#13;
```&#13;
&#13;
## 3.4 Creating a Simple UNet Model&#13;
&#13;
We’ll use a mini UNet model (not the standard one) that still achieves good results.&#13;
&#13;
```python&#13;
class BasicUNet(nn.Module):&#13;
    '''A minimal UNet implementation.'''&#13;
&#13;
    def __init__(self, in_channels=1, out_channels=1):&#13;
        super().__init__()&#13;
        self.down_layers = torch.nn.ModuleList(&#13;
            [&#13;
                nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),&#13;
                nn.Conv2d(32, 64, kernel_size=5, padding=2),&#13;
                nn.Conv2d(64, 64, kernel_size=5, padding=2),&#13;
            ]&#13;
        )&#13;
        self.up_layers = torch.nn.ModuleList(&#13;
            [&#13;
                nn.Conv2d(64, 64, kernel_size=5, padding=2),&#13;
                nn.Conv2d(64, 32, kernel_size=5, padding=2),&#13;
                nn.Conv2d(32, out_channels, kernel_size=5, padding=2),&#13;
            ]&#13;
        )&#13;
        self.act = nn.SiLU()  # Activation function&#13;
        self.downscale = nn.MaxPool2d(2)&#13;
        self.upscale = nn.Upsample(scale_factor=2)&#13;
&#13;
    def forward(self, x):&#13;
        h = []&#13;
        for i, l in enumerate(self.down_layers):&#13;
            x = self.act(l(x))  # Pass through the layer and activation function&#13;
            if i &lt; 2:  # Skip connection for all but the final down layer&#13;
                h.append(x)  # Store output for skip connection&#13;
                x = self.downscale(x)  # Downscale for next layer&#13;
&#13;
        for i, l in enumerate(self.up_layers):&#13;
            if i &gt; 0:  # For all but the first up layer&#13;
                x = self.upscale(x)  # Upscale&#13;
                x += h.pop()  # Use stored output (skip connection)&#13;
            x = self.act(l(x))  # Pass through the layer and activation function&#13;
&#13;
        return x&#13;
```&#13;
&#13;
The network looks like this:&#13;
&#13;
![Image here](https://i-blog.csdnimg.cn/direct/9d885023e67d43dd84c7e5b27bd770c9.png)&#13;
&#13;
## 3.5 Defining the Training Parameters&#13;
&#13;
```python&#13;
# Dataloader (adjust batch size)&#13;
batch_size = 128&#13;
train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)&#13;
&#13;
# Number of epochs&#13;
n_epochs = 30&#13;
&#13;
# Create the network&#13;
net = UNet2DModel(&#13;
    sample_size=28,  # Target image resolution&#13;
    in_channels=1,  # Input channels, 3 for RGB images&#13;
    out_channels=1,  # Output channels&#13;
    layers_per_block=2,  # ResNet layers per UNet block&#13;
    block_out_channels=(32, 64, 64),  # Matching our basic UNet example&#13;
    down_block_types=(&#13;
        'DownBlock2D',  # Regular ResNet downsampling block&#13;
        'AttnDownBlock2D',  # ResNet block with attention&#13;
        'AttnDownBlock2D',&#13;
    ),&#13;
    up_block_types=(&#13;
        'AttnUpBlock2D',&#13;
        'AttnUpBlock2D',  # ResNet block with attention&#13;
        'UpBlock2D',  # Regular ResNet upsampling block&#13;
    ),&#13;
)&#13;
net.to(device)&#13;
&#13;
# Loss function&#13;
loss_fn = nn.MSELoss()&#13;
&#13;
# Optimizer&#13;
opt = torch.optim.Adam(net.parameters(), lr=1e-3)&#13;
&#13;
# Track losses&#13;
losses = []&#13;
&#13;
# Training loop&#13;
for epoch in range(n_epochs):&#13;
    for x, y in train_dataloader:&#13;
        x = x.to(device)  # Data on GPU&#13;
        noise_amount = torch.rand(x.shape[0]).to(device)  # Random noise amount&#13;
        noisy_x = corrupt(x, noise_amount)  # Create noisy input&#13;
&#13;
        # Get model prediction&#13;
        pred = net(noisy_x, 0).sample  # Use timestep 0&#13;
&#13;
        # Calculate the loss&#13;
        loss = loss_fn(pred, x)  # Compare to original clean image&#13;
&#13;
        # Backprop and update parameters&#13;
        opt.zero_grad()&#13;
        loss.backward()&#13;
        opt.step()&#13;
&#13;
        losses.append(loss.item())&#13;
&#13;
    avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)&#13;
    print(f'Epoch {epoch}. Average loss: {avg_loss:.5f}')&#13;
```&#13;
&#13;
## 3.6 Testing the Model&#13;
&#13;
After training, we can test the diffusion model’s power:&#13;
&#13;
```python&#13;
n_steps = 40&#13;
x = torch.rand(8, 1, 28, 28).to(device)  # Start from random noise&#13;
step_history = [x.detach().cpu()]&#13;
pred_output_history = []&#13;
&#13;
for i in range(n_steps):&#13;
    with torch.no_grad():  # No gradients during inference&#13;
        pred = net(x,0).sample  # Predict denoised image&#13;
    pred_output_history.append(pred.detach().cpu())  # Store prediction&#13;
    mix_factor = 1 / (n_steps - i)  # Mix towards prediction&#13;
    x = x * (1 - mix_factor) + pred * mix_factor  # Move partway to the denoised image&#13;
    step_history.append(x.detach().cpu())  # Store for plotting&#13;
&#13;
fig, axs = plt.subplots(n_steps, 2, figsize=(9, 32), sharex=True)&#13;
axs[0, 0].set_title('Input (noisy)')&#13;
axs[0, 1].set_title('Model Prediction')&#13;
for i in range(n_steps):&#13;
    axs[i, 0].imshow(torchvision.utils.make_grid(step_history[i])[0].clip(0, 1), cmap='Greys')&#13;
    axs[i, 1].imshow(torchvision.utils.make_grid(pred_output_history[i])[0].clip(0, 1), cmap='Greys')&#13;
```&#13;
&#13;
![Image here](https://i-blog.csdnimg.cn/direct/84fa926fde4146a6812053b5d88f366b.png)&#13;
&#13;
This shows the final results of testing the model, and the output looks quite impressive!&#13;
&#13;
# Conclusion&#13;
&#13;
This concludes the explanation of the diffusion model. You can try modifying the UNet network or adjusting parameters to see if you can achieve even more remarkable results!。</description><guid isPermaLink="true">https://Refound-445.github.io/post/The%20Diffusion%20Principle%20Understood%20From%20The%20Perspective%20Of%20Convolution.html</guid><pubDate>Wed, 15 Jan 2025 18:15:50 +0000</pubDate></item></channel></rss>